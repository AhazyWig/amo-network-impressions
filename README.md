# AMO network impressios (Test Task)

## Завдання 1
Для реалізації цього завдання я обрав polars, а не pandas
У даному кейсі це кращий варіант, бо:
1. Polars значно швидше за Pandas при групуваннях та агрегаціях великих датасетів (завдяки Rust движку)
2. Polars багатопотоковий, на відміну від Pandas
3. Він краще використовує пам'ять через формат Arrow
Також хорошим варіантом є реалізація через PySpark (особливо для дуже великих датасетів), але для тестового кейсу Polars достатньо.

Посилання на реалізацію [Завдання 1](https://github.com/AhazyWig/amo-network-impressions/blob/main/task_1/task_1.ipynb).

## Завдання 2
Для вибору БД проаналізуємо вимоги до бази:
1. Дані надходять щогодини у вигляді файлів з великою кількістю рядків
2. Основна аналітика - BI репорти. Важливо: дохідність по продукту (Product), дохідність по пристрою (DeviceCategory), тренди по сесіях (session_id).
3. Важлива швидка агрегація та фільтрація по часу, продукту, девайсу.
4. Дані late-arriving events (рядки за попередні години можуть приходити пізніше)
5. Очікується інкрементальна обробка файлів без значного оновлення старих даних (в основному додавання).

Загалом, задача є типовою для OLAP системи (щогодинне завантаження, швидкий аналіз), тому тут підійде будь-яка OLAP сервіс (GCP, AWS Redshift, Snowflake тощо).
Для цієї задачі можемо обрати BigQuery, тому що це типова швидка OLAP, зручно працювати з великими даними, просте керування партиціюванням, повністю servless рішення, не потрібно займатися ручним тюнінгом при масштабуванні. 

### Підхід до задачі, моделювання даних та проектування таблиць
При подібних задачах у продакшн я би пропонував зберігати усі файли у бакеті (Cloud Storage, S3 для AWS) і для BI задач вивантажувати лише таблицю з потрібними даними.

У рамках цього тестового відразу візьмемо потрібні дані, вивантажимо їх як raw таблицю і окремо будемо агрегувати у ще одну окрему фінальну таблицю для BI.
#### Обгрунтування підходу до моделювання
Взагалі, можна підійти до моделювання таблиць 2 способами:
1. Використання OBT
2. Dimensional modelling (створення star/snowflake схеми)

У даному завданні я обрав використання OBT підходу:
1. Джерело даних одне, вся інформація надходить з одного сорса, тому немає потреби у складній схемі із кількома dim таблицями.
2. Аналіз фокусується лише на 3 основних атрибутах (Product, DeviceCategory, Session_id), усі основні метрики (EstimatedBackfillRevenue та к-сть показів) легко агрегуються і без джоінів, тому dim таблиці тут overwork.

Чому Star Schema не потрібні:
1. Реалізація SCD у даному кейсі не потрібна, ми чітко розуміємо, що Product та DeviceCategory статичні на момент показу реклами.
2. Star Schema була б доречна у випадку великої к-сті вимірів та складних зв'язків між ними. У даному кейсі такої бізнес-задачі немає.

#### Проектування таблиці (DDL)
Для raw таблиці структура наступна:
```
CREATE TABLE network_impressions_raw (
    session_id STRING,
    DeviceCategory STRING,
    Product STRING,
    event_time TIMESTAMP,
    EstimatedBackfillRevenue FLOAT64,
    ImpressionId STRING,
    dt DATE
)
PARTITION BY dt
CLUSTER BY Product, DeviceCategory;
```

Для побудови фінальної таблиці, яка враховує умови завдання по аналізу, можна використати таку структуру таблиці:
```
CREATE TABLE network_impressions_fact (
    session_id STRING,
    Product STRING,
    DeviceCategory STRING,
    dt DATE,
    total_cost FLOAT64,
    impression_count INT64
)
PARTITION BY dt
CLUSTER BY Product, DeviceCategory;

```
Якщо бізнесу необхідно буде в майбутньому по-іншому аналізувати покази реклами, то ми завжди можемо побудувати нову структуру під бізнес-потреби, використовуючи дані, які ми зберігаємо у bucket. У поточній реалізації дана структура таблиці повністю покриває потребу бізнесу бачити динаміку витрат в розрізі сесій, категорій, продуктів.

Чому партиціювання по днях:
1. Партиціювання по дані означає, що BQ фізично зберігає дані по сегментам дня. Це корисно, коли у нас буде дуже важка таблиця і при фільтрі за конкретний день читатиметься відповідна партиція, а не вся таблиця - це економніше
При late-arrive рядках рядки за попередні години або дні потраплятимуть у старі партиції, BQ під час аппенду буде оновлювати лише ті партиції, які відповідає даті рядка.
2. При партиціюванні погодинно вийде багато партицій, це погіршить продуктивність сканування даних та збільшить shuffle даних по кластерам для виконання запиту.

2.1 У BQ є ліміт +-10000 партицій на таблицю

Кластеризація по продукту і девайсу допоможе логічно розбити дані у кластерах та пришвидшить роботу запиту з використанням WHERE фільтрів на цих полях.

Фінальна таблиця:

| Колонка            | Тип       |
| ------------------ | --------- | 
| `session_id`       | STRING    |
| `Product`          | STRING    |
| `DeviceCategory`   | STRING    |
| `total_cost`       | FLOAT64   |
| `impression_count` | INT64     |
| `dt`               | DATE      |

Реалізація [Python скрипта](https://github.com/AhazyWig/amo-network-impressions/blob/main/task_2/bq_etl.py).

Реалізація включає:
1. Читання GZ файлу через Polars
2. Створення поля session_id (через sessionid або dfpsessionid)
3. Трансформація та агрегування даних
4. Завантаження сирих даних у raw таблицю
5. Завантаження та оновлення даних у fact таблиці

Для врахування late-arrive подій обрано таку реалізацію скрипта, де:
1. Python скрипт вивантажує усю інформацію (з мінімальною  обробкою, без агрегацій) у таблицю. Тут ми через APPEND будемо записувати і минулі події у відповідні партиції.
2. Фінальну агрегацію робимо робимо у fact таблиці, яка оновлюється через MERGE, щоб врахувати події, які "долетіли".

Що можна покращити для прод-варіанту:
1. Логування джоб у окрему таблицю
2. Після завантаження у бакет скриптом визначати, які дані нам прийшли і які партиції нам треба оновити
3. Якщо будуть проблеми з продуктивністю, можна переписати цей скрипт під PySpark джобу


### Альтернативний підхід до реалізації кейсу
#### Використання DBT для моделювання даних

Підхід DBT допомагає перейти від ETL процесу до ELT підходу, де усю трансформацію даних ми виконуємо обчислювальними потужностями самого DWH.

За допомогою DBT ми можемо завантажити наші дані "як є", без попередньої обробки (або з мінімальною обробкою), а далі уже консолідовано їх обробляти. Таким чином ми можемо поділити наше моделювання на кілька шарів:
1. Staging - наші сирі, необроблені дані
2. Intermediate - преоброблені дані, але не готові до аналізу
3. Marts - створені бізнес-сутності, агреговані таблиці для аналізу BI

Як і у варіанті із двома таблицями, у DBT можна реалізувати той самий підхід, де staging таблицею буде network_impressions_raw, а intermediate/marts буде network_impressions_fact. Не потрібно з нуля прописувати логіку MERGE/insert overwrite, вона доступна з-під коробки.
Додатково у DBT можемо використати тестування, jinja шаблонізації для побудови фінальних агрегацій. 

Плюсами такого підходу є:
1. Чітка структура трансформацій у SQL
2. Спрощення налаштування інкрементального оновлення, бо все доступно з-під коробки
3. Доступна можливість побудови data catalog, реалізації data governance з-під коробки
4. Можливість налаштувати обробку late-arrive events через інкрементальні моделі

Мінуси (скоріше як складнощі у підході, ніж мінуси у реалізації):
1. Не обробляє великі об'єми "на льоту" - dbt працює batch методом
2. Потрібна інфраструктура для розгортання або dbt Cloud

#### Використання Spark/PySpark
PySpark добре підходить для обробки великих потоків даних або складних трансформацій.

Плюси:
1. Масштабованість PySpark допомагає легко обробляти великі масиви даних на кластерах, без проблем із пам'яттю може обробляти великі CSV/GZ файли, які надходять щогодини.
2. У плані трансформації також гнучкний, у джобі відразу можна робити будь-які обчислення на рівні рядків, групування, джоіни тощо
3. Можна використовувати як і для Batch ETL так і для streaming процессингу

Мінуси:
1. Є певна складність налаштування, порівняно з Polars або Pandas скриптами

Отже, можна зробити наступний висновок:
1. Використовувати DBT варто для побудови структурованої ELT трансформацій.
2. Polars + BigQuery - швидка реалізація вивантаження даних скриптом, при великих даних будуть проблеми з продуктивністю
3. PySpark підійде як стабільне продове рішення для обробки великих даних та для складних трансформацій. Якщо даних дуже багато - я би обрав саме цей варіант, особливо якщо стек S3 + Glue.
